{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imports e configuração"
      ],
      "metadata": {
        "id": "8lN8OuHXIoK3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEPzjMtnfxD-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import nltk\n",
        "import psycopg2\n",
        "import psycopg2.extras\n",
        "from fastapi import FastAPI, UploadFile, File, HTTPException\n",
        "from fastapi.responses import HTMLResponse, JSONResponse\n",
        "from pydantic import BaseModel\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "from langchain_unstructured import UnstructuredLoader\n",
        "from dotenv import load_dotenv\n",
        "import uvicorn\n",
        "\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "app = FastAPI()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conexão com o banco de dados e criação de tabelas"
      ],
      "metadata": {
        "id": "dGyM7asyIv9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def connect_db():\n",
        "    try:\n",
        "        return psycopg2.connect(\n",
        "            host=os.getenv(\"PG_HOST\", \"localhost\"),\n",
        "            dbname=os.getenv(\"PG_DB\", \"pgvector_db\"),\n",
        "            user=os.getenv(\"PG_USER\", \"LFP\"),\n",
        "            password=os.getenv(\"PG_PASS\", \"root\"),\n",
        "            port=int(os.getenv(\"PG_PORT\", 5432))\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(\"Erro ao conectar ao banco de dados:\", e)\n",
        "        raise\n",
        "\n",
        "def create_tables():\n",
        "    conn = connect_db()\n",
        "    try:\n",
        "        with conn.cursor() as cur:\n",
        "            cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
        "            cur.execute(\"\"\"\n",
        "                CREATE TABLE IF NOT EXISTS propositions (\n",
        "                    id SERIAL PRIMARY KEY,\n",
        "                    proposition TEXT NOT NULL,\n",
        "                    embedding vector(1536)\n",
        "                );\n",
        "            \"\"\")\n",
        "            conn.commit()\n",
        "    except Exception as e:\n",
        "        print(\"Erro ao criar tabelas:\", e)\n",
        "    finally:\n",
        "        conn.close()\n",
        "\n",
        "def create_chat_history_table():\n",
        "    conn = connect_db()\n",
        "    try:\n",
        "        with conn.cursor() as cur:\n",
        "            cur.execute(\"\"\"\n",
        "                CREATE TABLE IF NOT EXISTS chat_history (\n",
        "                    id SERIAL PRIMARY KEY,\n",
        "                    user_message TEXT NOT NULL,\n",
        "                    bot_response TEXT NOT NULL,\n",
        "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        "                );\n",
        "            \"\"\")\n",
        "            conn.commit()\n",
        "    except Exception as e:\n",
        "        print(\"Erro ao criar tabela de histórico:\", e)\n",
        "    finally:\n",
        "        conn.close()\n",
        "\n",
        "create_tables()\n",
        "create_chat_history_table()"
      ],
      "metadata": {
        "id": "UcDtq2uLIyKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuração do modelo"
      ],
      "metadata": {
        "id": "xLCLnCUJJB2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, openai_api_key=openai_api_key)\n",
        "    embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=openai_api_key)\n",
        "except Exception as e:\n",
        "    print(\"Erro ao inicializar os modelos da OpenAI:\", e)\n",
        "    raise"
      ],
      "metadata": {
        "id": "_RrpvA7PJDv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funções utilitárias para vetorização, decomposição e manipulação de dados"
      ],
      "metadata": {
        "id": "r9pbVCkUJEeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_similar_propositions(conn, question_embedding, top_k=3, distance_threshold=0.5):\n",
        "    with conn.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:\n",
        "        embedding_str = \"[\" + \",\".join(map(str, question_embedding)) + \"]\"\n",
        "        cur.execute(\"\"\"\n",
        "            SELECT proposition, embedding <=> %s::vector AS distance\n",
        "            FROM propositions\n",
        "            ORDER BY distance\n",
        "            LIMIT %s;\n",
        "        \"\"\", (embedding_str, top_k))\n",
        "        results = cur.fetchall()\n",
        "    return [row for row in results if row['distance'] <= distance_threshold]\n",
        "\n",
        "def save_chat_history(conn, user_msg, bot_resp):\n",
        "    with conn.cursor() as cur:\n",
        "        cur.execute(\"\"\"\n",
        "            INSERT INTO chat_history (user_message, bot_response)\n",
        "            VALUES (%s, %s);\n",
        "        \"\"\", (user_msg, bot_resp))\n",
        "        conn.commit()\n",
        "\n",
        "def load_chat_history(conn, limit=50):\n",
        "    with conn.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:\n",
        "        cur.execute(\"\"\"\n",
        "            SELECT user_message, bot_response, created_at\n",
        "            FROM chat_history\n",
        "            ORDER BY created_at ASC\n",
        "            LIMIT %s;\n",
        "        \"\"\", (limit,))\n",
        "        return cur.fetchall()\n",
        "\n",
        "def load_and_clean_text(file_path):\n",
        "    loader = UnstructuredLoader(file_path)\n",
        "    docs = loader.load()\n",
        "    full_text = \" \".join([doc.page_content for doc in docs])\n",
        "    cleaned_text = \" \".join(line.strip() for line in full_text.splitlines() if line.strip())\n",
        "    return cleaned_text\n",
        "\n",
        "def split_into_sentences(text):\n",
        "    return nltk.tokenize.sent_tokenize(text)\n",
        "\n",
        "def decompose_sentence(sentence, chat):\n",
        "    system_prompt = \"\"\"\n",
        "Decompose the \"Content\" into clear and simple propositions, ensuring they are interpretable out of context.\n",
        "\n",
        "1. Split compound sentence into simple sentences...\n",
        "...\n",
        "    \"\"\".strip()\n",
        "\n",
        "    try:\n",
        "        system_message = SystemMessage(content=system_prompt.format(content=sentence))\n",
        "        human_message = HumanMessage(content=\"\")\n",
        "        response = chat([system_message, human_message])\n",
        "        text = response.content.strip()\n",
        "        if text.startswith(\"```json\"): text = text[len(\"```json\"):].strip()\n",
        "        if text.endswith(\"```\"): text = text[:-3].strip()\n",
        "        propositions = json.loads(text)\n",
        "    except Exception:\n",
        "        return [sentence]\n",
        "\n",
        "    return [p.strip() for p in propositions if p.strip()]\n",
        "\n",
        "def insert_embedding(conn, proposition, embedding):\n",
        "    with conn.cursor() as cur:\n",
        "        embedding_str = \"[\" + \",\".join(map(str, embedding)) + \"]\"\n",
        "        cur.execute(\"\"\"\n",
        "            INSERT INTO propositions (proposition, embedding)\n",
        "            VALUES (%s, %s::vector);\n",
        "        \"\"\", (proposition, embedding_str))\n",
        "        conn.commit()"
      ],
      "metadata": {
        "id": "wMXrWGAIJHQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelos e rotas FastAPI"
      ],
      "metadata": {
        "id": "9wH5THH0JQBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UserMessage(BaseModel):\n",
        "    message: str\n",
        "\n",
        "@app.get(\"/\", response_class=HTMLResponse)\n",
        "async def index():\n",
        "    return \"<h1>Chatbot com Upload</h1>\"\n",
        "\n",
        "@app.get(\"/history\")\n",
        "async def get_history():\n",
        "    try:\n",
        "        conn = connect_db()\n",
        "        history = load_chat_history(conn)\n",
        "        return JSONResponse(content=[{\n",
        "            \"user_message\": h[\"user_message\"],\n",
        "            \"bot_response\": h[\"bot_response\"],\n",
        "            \"created_at\": h[\"created_at\"].isoformat()\n",
        "        } for h in history])\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Erro ao carregar histórico: {e}\")\n",
        "    finally:\n",
        "        if conn: conn.close()\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "async def chat_endpoint(user_msg: UserMessage):\n",
        "    try:\n",
        "        user_input = user_msg.message\n",
        "        conn = connect_db()\n",
        "        question_emb = embeddings_model.embed_query(user_input)\n",
        "        results = search_similar_propositions(conn, question_emb)\n",
        "\n",
        "        if not results:\n",
        "            bot_response = \"Desculpe, não encontrei respostas relevantes.\"\n",
        "        else:\n",
        "            context_texts = \"\\n\".join([f\"- {row['proposition']}\" for row in results])\n",
        "            system_prompt = f\"\"\"\n",
        "Você é um assistente que responde perguntas baseando-se unicamente nas informações listadas abaixo...\n",
        "\n",
        "{context_texts}\n",
        "\n",
        "Pergunta: {user_input}\n",
        "\n",
        "Resposta:\"\"\"\n",
        "            response = chat([SystemMessage(content=system_prompt), HumanMessage(content=\"\")])\n",
        "            bot_response = response.content\n",
        "\n",
        "        save_chat_history(conn, user_input, bot_response)\n",
        "        return {\"response\": bot_response}\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Erro durante o chat: {e}\")\n",
        "    finally:\n",
        "        if conn: conn.close()\n",
        "\n",
        "@app.post(\"/uploadfile\")\n",
        "async def upload_file(file: UploadFile = File(...)):\n",
        "    if not file.filename:\n",
        "        raise HTTPException(status_code=400, detail=\"Arquivo não enviado\")\n",
        "\n",
        "    temp_path = f\"temp_{file.filename}\"\n",
        "    try:\n",
        "        with open(temp_path, \"wb\") as buffer:\n",
        "            buffer.write(await file.read())\n",
        "\n",
        "        text = load_and_clean_text(temp_path)\n",
        "        sentences = split_into_sentences(text)\n",
        "        conn = connect_db()\n",
        "        count = 0\n",
        "        for sentence in sentences:\n",
        "            decomposed_props = decompose_sentence(sentence, chat)\n",
        "            for prop in decomposed_props:\n",
        "                if prop:\n",
        "                    emb = embeddings_model.embed_query(prop)\n",
        "                    insert_embedding(conn, prop, emb)\n",
        "                    count += 1\n",
        "        return {\"message\": f\"{count} proposições vetorizadas com sucesso.\"}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Erro ao processar o arquivo: {e}\")\n",
        "    finally:\n",
        "        if os.path.exists(temp_path): os.remove(temp_path)\n",
        "        if conn: conn.close()"
      ],
      "metadata": {
        "id": "N0tttWS1JQaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executar servidor"
      ],
      "metadata": {
        "id": "TtRNIxm9JWf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(\"chatbot:app\", host=\"127.0.0.1\", port=8000, reload=True)"
      ],
      "metadata": {
        "id": "vSQA4rfkJZEe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}